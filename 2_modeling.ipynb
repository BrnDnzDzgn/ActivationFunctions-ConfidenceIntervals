{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b380bda1",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b53b8d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "DataLoaders loaded successfully!\n",
      "Training batches: 625\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# Check device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "with open('cifar10_loaders.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "train_loader = data['train_loader']\n",
    "val_loader = data['val_loader']\n",
    "test_loader = data['test_loader']\n",
    "class_names = data['class_names']\n",
    "batch_size = data['batch_size']\n",
    "\n",
    "print(\"DataLoaders loaded successfully!\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b8213",
   "metadata": {},
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98510414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING NETWORK ARCHITECTURE\n",
      "============================================================\n",
      "âœ… Network test successful!\n",
      "  Input shape: torch.Size([4, 3, 32, 32])\n",
      "  Output shape: torch.Size([4, 10])\n",
      "  Total parameters: 323,498\n",
      "\n",
      "ðŸ“Š Testing different configurations:\n",
      "  relu         + None    : 323,498 parameters\n",
      "  relu         + dropout : 323,498 parameters\n",
      "  leaky_relu   + None    : 323,498 parameters\n",
      "  leaky_relu   + dropout : 323,498 parameters\n",
      "  elu          + None    : 323,498 parameters\n",
      "  elu          + dropout : 323,498 parameters\n",
      "  tanh         + None    : 323,498 parameters\n",
      "  tanh         + dropout : 323,498 parameters\n"
     ]
    }
   ],
   "source": [
    "class CIFAR10Net(nn.Module):\n",
    "    \"\"\"Flexible CNN for CIFAR-10 with configurable activation and regularization\"\"\"\n",
    "    \n",
    "    def __init__(self, activation_name='relu', regularization=None, dropout_rate=0.5):\n",
    "        super(CIFAR10Net, self).__init__()\n",
    "        \n",
    "        # Store configuration\n",
    "        self.activation_name = activation_name\n",
    "        self.regularization = regularization\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Classifier\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        \n",
    "        # Dropout (if used)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def get_activation(self):\n",
    "        \"\"\"Return activation function based on name\"\"\"\n",
    "        if self.activation_name == 'relu':\n",
    "            return F.relu\n",
    "        elif self.activation_name == 'leaky_relu':\n",
    "            return F.leaky_relu\n",
    "        elif self.activation_name == 'elu':\n",
    "            return F.elu\n",
    "        elif self.activation_name == 'tanh':\n",
    "            return torch.tanh\n",
    "        else:\n",
    "            return F.relu\n",
    "    \n",
    "    def forward(self, x):\n",
    "        act = self.get_activation()\n",
    "        \n",
    "        # Block 1\n",
    "        x = act(self.bn1(self.conv1(x)))\n",
    "        x = act(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        if self.regularization == 'dropout':\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        # Block 2\n",
    "        x = act(self.bn3(self.conv3(x)))\n",
    "        x = act(self.bn4(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "        if self.regularization == 'dropout':\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        # Block 3\n",
    "        x = act(self.bn5(self.conv5(x)))\n",
    "        x = act(self.bn6(self.conv6(x)))\n",
    "        x = self.pool3(x)\n",
    "        if self.regularization == 'dropout':\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        # Classifier\n",
    "        x = self.gap(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = act(self.fc1(x))\n",
    "        if self.regularization == 'dropout':\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test the network\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING NETWORK ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_net = CIFAR10Net(activation_name='relu', regularization='dropout').to(device)\n",
    "test_input = torch.randn(4, 3, 32, 32).to(device)\n",
    "test_output = test_net(test_input)\n",
    "\n",
    "print(f\"Network test successful!\")\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in test_net.parameters()):,}\")\n",
    "\n",
    "# Test different configurations\n",
    "print(\"\\nTesting different configurations:\")\n",
    "for activation in ['relu', 'leaky_relu', 'elu', 'tanh']:\n",
    "    for reg in [None, 'dropout']:\n",
    "        net = CIFAR10Net(activation_name=activation, regularization=reg).to(device)\n",
    "        params = sum(p.numel() for p in net.parameters())\n",
    "        print(f\"  {activation:12s} + {str(reg):8s}: {params:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7333f9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db084ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=15, lr=0.001, weight_decay=0):\n",
    "    \"\"\"\n",
    "    Train a model and return history with timing information\n",
    "    \"\"\"\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Learning rate scheduler (optional - reduces LR when plateau is reached)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'epoch_time': []\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Add L2 regularization if specified (and not using optimizer weight_decay)\n",
    "            if model.regularization == 'l2' and weight_decay == 0:\n",
    "                l2_reg = 0\n",
    "                for param in model.parameters():\n",
    "                    l2_reg += torch.norm(param, 2)\n",
    "                loss += 0.001 * l2_reg\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Store history\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f'Epoch {epoch+1:2d}/{epochs} | '\n",
    "                f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | '\n",
    "                f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | '\n",
    "                f'Time: {epoch_time:.2f}s')\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f'\\nTraining completed in {total_time:.2f}s ({total_time/60:.2f} minutes)')\n",
    "    \n",
    "    # Add final test accuracy to history\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    history['test_acc'] = 100. * test_correct / test_total\n",
    "    print(f'Test Accuracy: {history[\"test_acc\"]:.2f}%')\n",
    "    \n",
    "    return history\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
